1. Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]

The goal of this project was to use machine learning algorithms to identify POIs from the dataset. In this project, POIs were those that commited fraud during the Enron fraud scandal of the early 2000s. Machine learning was helpful here as we could make use of supervised learning techniques to optimise our alrogithms to hone in on POIs. The dataset contained 146 individuals and 20 features on each, although many data points were missing (ranging between 21 for total_payments and 142 for loan_advances). Given the vast number of data points in the dataset, using machine learning allows us to create accurate classifiers based on large amounts of data. For example, my classifier has quantified the link between exercised stock options and the likelyhood of having committed fraud at Enron. By training our algorithms on this data, we can hopefully find trends between these variables that stand true in other companies/ organisiations.

There were outliers in this dataset. I removed 6 observations based on this. My approach was first to visualize the distribution of each feature (graph_feature_distribution). From this, it became clear that there was an observation that did not belong in our analysis: TOTAL. I decided to take a programmatic approach to the outliers. Using an "outer-fence" methodology, for each feature i was analysing, i removed any observation whose value was 10 times the IQR greater than Q3 (Q3 + 10 * IQR) or 10 times the IQR lower than Q1 (Q1 - 10 * Q3). Whilst this fence is significantly wider than typical, i intentially set it wide in order to ensure i only removed huge outliers. With this methodology i removed a further 5 observations, 6 in total. I think this is correct as given the extreme nature of thier results, i do not think their behaviour typical of the dataset.


2. What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]

In my final POI identifier, i used salary, bonus, exercised stock options and restricted stock. I began by focusing my analysis on a few features: salary, bonus, expenses and director fees. My intuition was that these would be the most significant factors in finding fraudsters. Also, whilst creating the script, it as easier to have fewer features. In the end i added exercised stock options, long term incentive, restricted stock and other to my list of ready-made features (the email features i incorporated into my own features). As part of my GridSearch/ pipeline i used a SelectKBest classifier to help narrow down my variables.

I did perform some scaling. As part of my classifier pipeline i used a MinMax sclarar. However, as my final classifier was a Decision Tree, the scaling did not have an effect.

I created a few new features to aid my analysis. With the email features, i analysed what affect the proportion of emails coming to or from POIs had. I also looked at a composite figure - what proportion of all emails sent or recieved were either to, from or were shared recipients with POIs. I thought this measure would be more accurate than either "to" or "from" alone. However, none of these had significant preidcting power. From the financial features, i also looked at the proportion of stock a person had exercised against total value. My intuition was that those commiting fraud would exercise lots of stock, knowing that the comapny wasn't sustainable in the long run. However, this wasn't significant either.

My final features and feature importances were: 

salary 					0.146939997617
bonus 					0.392571765759
exercised_stock_options 0.303379462679
restricted_stock 		0.157108773945

For SelectKBest i used k=4. I used this after trail-and-error with my GridSearch/ pipeline. I believe that 4 is an appropriate number. I wanted a low number, with features falling into two categories (financial and email) there was likely to be a high degree of multicolinearity between features and i didn't want this too prevelant in my results. I tested a PCA approach, however, i feel this was contributing to an overfitting of the data, as it was increasing my f1 score on the test data, but severly reducing results when running tester.py.


3. What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]

I ended up using a Decision Tree with a minimum sample split of 14, presorted, having selected the 4 top features. I selected my model programmatically, using a pipeline/ GridSearch for the Decision Tree, SVM and Naive Bayes and selecting the classifier with the highest f1 score. I trialed an AdaBoost classifier, but again i felt tht with the small sample size it was overfitting my data (even when allowing the learnign rate to vary) - as the test f1 score was rising (i managed f1 = 1) whilst worsening my tester.py results.


4. What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric item: “tune the algorithm”]

Tuning is the process of modifying the parameters of your algorithm in order to best reflect the trends in the data. We can enhance our fit to the data, but may also need to reduce accuracy in order to prevent overfitting. I found avoiding this overfitting difficult in this project, given the small dataset. Plugging numbers through the GridSearch had limited success as maximising my training f1 score often brought about a lower test f1 score (due to overfitting). As part of my tuning i trailed PCA techniques, but decided not to include them in the final version. I also tuned the minimum number of samples needed in a leaf to split it, as well as whether to presort the data. 

5. What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric item: “validation strategy”]

Validation is testing your results to see if they hold true when applied to larger/ more general datasets. A classic mistake if doen wrong is to think that your model has far more predictive power than it really does as you overfit your data. I validated my analysis by running my optimised model through tester.py and checking my precision/ recall/ f1 scores on that data. In my final model, when running through tester.py i achieve a precision score of 0.34 and a recall score of 0.31. A precision score of 0.34 means that given that my algorithm has made a prediction of a Positive result (the person is a fraudster), there is a 34% chance that they actually are a fraudster. A recall score of 0.31 means that given that a person is a fraudster, my algorithm has a 31% chance of predicting them as a POI.


6. Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]
